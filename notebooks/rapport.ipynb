{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957bdb9b",
   "metadata": {},
   "source": [
    "# Multi-Agent Reinforcement Learning (Met DQN)\n",
    "Voor deze opdracht gaan wij een Multi-Agent Reinforcement Learning algoritme toe passen voor het spel Warlords\n",
    "\n",
    "### Warlords\n",
    "\n",
    "Warlords verscheen in 1980 voor de Atari 2600<sup>[[1]](https://en.wikipedia.org/wiki/Warlords_(1980_video_game))</sup>. Het spel is een klassieker binnen het arcade-genre en staat bekend om zijn unieke combinatie van actie, strategie en multiplayer-competitie. Elke speler verdedigt zijn eigen kasteel in een van de vier hoeken van het scherm, terwijl een vuurbal continu over het speelveld beweegt. Door het schild te bewegen, kan een speler de vuurbal afweren, terugkaatsen of zelfs richten op de kastelen van tegenstanders.\n",
    "\n",
    "Het doel is om de muren van de andere kastelen te vernietigen en uiteindelijk de koning van de tegenstander te raken, terwijl je je eigen kasteel zo lang mogelijk beschermt. De gameplay vereist snelle reflexen, strategisch inzicht en het vermogen om te anticiperen op de acties van andere spelers. Warlords wordt vaak geprezen als een vroege vorm van multiplayer battle royale, waarbij interactie en competitie tussen spelers centraal staan. De eenvoudige besturing en het verslavende spelmechanisme maken het tot een tijdloze klassieker die nog steeds wordt bestudeerd in onderzoek naar multi-agent systemen en kunstmatige intelligentie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa5d9c",
   "metadata": {},
   "source": [
    "\n",
    "## Probleemstelling\n",
    "\n",
    "Het centrale vraagstuk in dit project betreft de ontwikkeling en evaluatie van een effectieve autonome agent binnen de competitieve, dynamische multi-agent omgeving van het Atari-spel **\"Warlords\"**.\n",
    "\n",
    "De kern van de probleemstelling wordt gevormd door de volgende elementen:\n",
    "\n",
    "*   **De Complexiteit van de Multi-Agent Dynamiek:**\n",
    "    *   De **\"Warlords\"**-omgeving vereist dat een agent niet alleen reageert op de spelstaat, maar ook anticipeert op en strategisch omgaat met de acties van andere, gelijktijdig opererende en concurrerende agenten.\n",
    "    *   Het succes van een agent hangt dus direct af van hoe goed hij interacteert en concurreert met andere agenten, in een systeem waar ze elkaar voortdurend beïnvloeden.\n",
    "\n",
    "*   **Doelstelling: Ontwikkeling van een Competitieve Agent:**\n",
    "    *   De primaire doelstelling is het construeren van een agent die in staat is om een hoog prestatieniveau te bereiken in **\"Warlords\"**.\n",
    "    *   Dit vereist de ontwikkeling van een agent die, door te leren van interactie met de omgeving, een strategie ontdekt en verfijnt die significant effectiever is dan de strategieën van agenten die opereren op basis van vooraf gedefinieerde, eenvoudige heuristieken of willekeurige acties.\n",
    "\n",
    "*   **Definitie van de Agent's Actieruimte:**\n",
    "    *   De agent moet op elk relevant tijdstip een beslissing nemen uit een discrete set van mogelijke acties die beschikbaar zijn in **\"Warlords\"**. Deze acties omvatten:\n",
    "        *   Laterale verplaatsing van het door de agent bestuurde verdedigingsschild (links/rechts).\n",
    "        *   Het stationair houden van dit schild.\n",
    "        *   Het uitvoeren van een vuuractie om de fortificaties van opponenten te treffen en de eigen basis te verdedigen.\n",
    "\n",
    "*   **De Perceptuele Uitdaging: Observatie via Visuele Data:**\n",
    "    *   Een significante uitdaging ligt in de wijze waarop de agent de spelomgeving waarneemt. De agent heeft uitsluitend toegang tot de spelstaat via een continue stroom van **RGB-pixeldata** (ruwe beelden van het spelscherm).\n",
    "    *   Uit deze hoog-dimensionale visuele input moet de agent zelfstandig alle relevante informatie extraheren, zoals de posities van de eigen entiteit, tegenstanders, projectielen, en de status van de verdedigingswerken.\n",
    "\n",
    "*   **Noodzaak voor Adaptief Leren van Complexe Strategieën:**\n",
    "    *   De ambitie is om een agent te realiseren die, uitsluitend op basis van deze visuele perceptie en interactie met de omgeving, complexe en adaptieve strategieën kan ontwikkelen.\n",
    "    *   Dit onderscheidt de beoogde agent van systemen die vertrouwen op vooraf gedefinieerde regels of vereenvoudigde representaties van de spelstaat, en beoogt de potentie aan te tonen van het leren van effectief gedrag in een visueel complexe, competitieve multi-agent setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab306d0",
   "metadata": {},
   "source": [
    "## Aanpak\n",
    "\n",
    "Om een effectieve autonome agent voor de \"Warlords\" omgeving te ontwikkelen, zal een Deep Q-Network (DQN) algoritme worden geïmplementeerd en geëvalueerd. Deze keuze is gebaseerd op het vermogen van DQN om complexe strategieën te leren direct vanuit hoog-dimensionale sensorische input, zoals de pixeldata van een game.\n",
    "\n",
    "### Uitleg van Deep Q-Networks (DQN)\n",
    "\n",
    "Een Deep Q-Network is een type Reinforcement Learning algoritme dat een neuraal netwerk gebruikt om de optimale actie-waardefunctie, bekend als *Q(s,a)*, te benaderen. Deze functie schat de verwachte totale toekomstige beloning (return) wanneer actie *a* wordt ondernomen in staat *s*, en vervolgens een optimale strategie wordt gevolgd.\n",
    "\n",
    "**Hoe DQN Werkt:**\n",
    "De kern van de DQN-aanpak, zoals geïmplementeerd in dit project, omvat de volgende sleutelcomponenten:\n",
    "\n",
    "1.  **Convolutional Neural Network (CNN) als Q-Functie Approximator (`DQN_CNN`):**\n",
    "    *   **Input:** De agent ontvangt de spelstatus als een stapel van meerdere gameframes<sup>[[2]](https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-network#:~:text=Why%20do%20we,those%20frames)</sup> (bijvoorbeeld 4 opeenvolgende frames). Deze frames worden omgezet naar grijswaarden, verkleind (bijv. naar 84x84 pixels) en genormaliseerd. Het stapelen van frames stelt het netwerk in staat om temporele informatie, zoals de snelheid en richting van objecten, af te leiden.\n",
    "    *   **Architectuur:** Een diep convolutioneel neuraal netwerk (CNN) wordt gebruikt om relevante ruimtelijke kenmerken uit de pixeldata te extraheren. Dit netwerk bestaat typisch uit:\n",
    "        *   Meerdere convolutionele lagen (`nn.Conv2d`) met learnable filters (kernels) en strides, die patronen zoals de positie van de bal, de paddles en de muren detecteren. De output van deze lagen wordt geactiveerd met een niet-lineaire activatiefunctie zoals ReLU (`F.relu`).\n",
    "        *   Na de convolutionele lagen worden de feature maps \"geflattened\" (uitgevlakt tot een eendimensionale vector).\n",
    "        *   Eén of meer volledig verbonden lagen (`nn.Linear`) verwerken deze vector om de uiteindelijke Q-waarden te produceren.\n",
    "    *   **Output:** Het netwerk produceert een vector van Q-waarden, één voor elke mogelijke discrete actie die de agent kan ondernemen in de gegeven staat.\n",
    "\n",
    "2.  **Experience Replay (`ReplayBuffer`):**\n",
    "    *   Om de training te stabiliseren en de efficiëntie van datagebruik te verhogen, wordt gebruik gemaakt van een *replay buffer*<sup>[[3]](https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm#:~:text=go%20through%20them!-,Experience%20Replay%20to%20make%20more%20efficient%20use%20of%20experiences,observation%20sequences%20and%20avoid%C2%A0action%20values%20from%20oscillating%20or%20diverging%20catastrophically.,-In%20the%20Deep)</sup>.\n",
    "    *   Tijdens de interactie met de omgeving slaat de agent transities op – bestaande uit de huidige staat (*s*), de genomen actie (*a*), de ontvangen beloning (*r*), de volgende staat (*s'*) en een indicator of de episode is beëindigd (*done*) – in deze buffer (een `collections.deque` met een vaste capaciteit).\n",
    "    *   Voor elke trainingsstap wordt een willekeurige mini-batch van deze opgeslagen transities uit de buffer gesampled. Dit doorbreekt de temporele correlaties tussen opeenvolgende ervaringen, wat leidt tot stabielere en effectievere training.\n",
    "\n",
    "3.  **Target Network:**\n",
    "    *   DQN maakt gebruik van twee neurale netwerken: een *policy network* (θ) dat actief wordt getraind en gebruikt wordt om acties te selecteren, en een *target network* (θ′) dat een periodiek bijgewerkte, \"bevroren\" kopie is van het policy network.<sup>[[4]](https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae#:~:text=Key%20Concepts%20of,to%20Q%2DLearning%3A)</sup>\n",
    "    *   Het target network wordt gebruikt om de target Q-waarden te berekenen tijdens de Bellman update. Het gebruik van een apart, stabieler target network helpt oscillaties en divergentie tijdens de training te voorkomen. Het target network wordt typisch elke *N* stappen gesynchroniseerd met de gewichten van het policy network (`self.target_net.load_state_dict(self.policy_net.state_dict())`).\n",
    "\n",
    "4.  **ε-Greedy Exploratie (`act` methode):**\n",
    "    *   Om een balans te vinden tussen het exploiteren van reeds geleerde kennis en het exploreren van nieuwe acties, wordt een ε-greedy strategie toegepast.<sup>[[5]](https://medium.com/@heyamit10/deep-reinforcement-learning-with-experience-replay-1222ea711897#:~:text=Balancing%20Exploration%20and,agent%E2%80%99s%20policy%20converges)</sup>\n",
    "    *   Met een kans ε kiest de agent een willekeurige actie. Met kans 1-ε kiest de agent de actie met de hoogste geschatte Q-waarde volgens het huidige policy network (`q_values.argmax(dim=1).item()`).\n",
    "    *   De waarde van ε wordt doorgaans gedurende de training verlaagd (bijvoorbeeld exponentieel van een hoge startwaarde naar een lage eindwaarde), zodat de agent in het begin meer exploreert en later meer exploiteert.\n",
    "\n",
    "5.  **Trainingsproces (`train_step` methode):**\n",
    "    *   Uit de replay buffer wordt een mini-batch van transities gesampled.\n",
    "    *   Voor elke transitie in de batch:\n",
    "        *   Het policy network berekent de Q-waarden voor de huidige staat *s*: *Q(s,a; θ)*.\n",
    "        *   Het target network berekent de Q-waarde van de beste actie in de volgende staat *s'*: *max<sub>a'</sub> Q(s',a'; θ′)*.\n",
    "        *   De Bellman target-waarde wordt berekend als: *y = r + γ · max<sub>a'</sub> Q(s',a'; θ′)* (waarbij γ de discount factor is). Als *s'* een terminale staat is, is *y = r*.\n",
    "    *   Een loss-functie, zoals Huber loss (`F.smooth_l1_loss`) of Mean Squared Error, meet het verschil tussen de voorspelde Q-waarde *Q(s,a; θ)* (voor de actie die daadwerkelijk in de transitie is genomen) en de target-waarde *y*.\n",
    "    *   Deze loss wordt gebruikt om de gewichten van het policy network (θ) bij te werken via backpropagation en een optimalisatie-algoritme zoals Adam (`optim.Adam`).\n",
    "\n",
    "**Onderscheidend Vermogen en Geschiktheid voor het Probleem:**\n",
    "DQN onderscheidt zich van klassieke RL-methoden en andere machine learning technieken doordat het direct kan leren van hoog-dimensionale, ruwe sensorische input (zoals game-pixels) zonder de noodzaak van handmatige feature engineering. Het is ontworpen om complexe, niet-lineaire relaties tussen staten en actiewaarden te leren. Dit is afgeleid uit deze paper<sup>[[6]](https://arxiv.org/pdf/1312.5602)</sup> waar in \"5.3 Main Evaluation\" de resultaten vergeleken worden met andere klassiekere ML technieken en DQN beter presteert.\n",
    "\n",
    "Voor het \"Warlords\" probleem, waarbij de input bestaat uit RGB-beelden en de agent strategische beslissingen moet nemen in een dynamische multi-agent omgeving, biedt DQN een krachtig raamwerk. Hoewel DQN oorspronkelijk voor single-agent omgevingen is ontworpen, kan het als basis dienen voor multi-agent scenario's, bijvoorbeeld door elke agent als een onafhankelijke DQN-learner te behandelen (Independent Q-Learning).\n",
    "\n",
    "**Verwachte Resultaten (Generiek):**\n",
    "De verwachting is dat een goed getrainde DQN-agent in staat zal zijn om geavanceerde, niet-deterministische strategieën te ontwikkelen die verder gaan dan wat met eenvoudige, vooraf gedefinieerde regels mogelijk is. Dit omvat het leren van timing, het anticiperen op de bewegingen van projectielen en mogelijk zelfs het impliciet modelleren van het gedrag van tegenstanders (in een multi-agent setting). De agent zou patronen moeten herkennen en zijn acties daarop moeten aanpassen om de cumulatieve beloning te maximaliseren.\n",
    "\n",
    "#### Experimenten met Hyperparameters\n",
    "\n",
    "Een cruciaal onderdeel van het ontwikkelen van een effectieve DQN-agent is het afstemmen van de hyperparameters. Daarom zal in dit project systematisch worden geëxperimenteerd met verschillende instellingen om hun invloed op de leerprestaties en de uiteindelijke effectiviteit van de agent te onderzoeken. De volgende sleutel-hyperparameters zullen worden gevarieerd:<sup>[[7]](https://www.numberanalytics.com/blog/optimizing-dqn-models-in-rl#hyperparameter-tuning-strategies:~:text=1.%20Hyperparameter%20Tuning,long%E2%80%90term%20gain)</sup>\n",
    "\n",
    "* **Learning rate (α):** Bepaalt de stapgrootte bij het updaten van de netwerkgewichten tijdens backpropagation; een goed gekozen waarde voorkomt zowel trage convergentie als numerische instabiliteit.  \n",
    "* **Batch size:** Geeft aan hoeveel transities per trainingsstap uit de replay-buffer worden gehaald; dit beïnvloedt de ruis in de gradienten én de geheugenefficiëntie.  \n",
    "* **Update frequency:** Hoe vaak de gewichten van het target-netwerk worden gesynchroniseerd met het policy-netwerk; speelt een cruciale rol in de stabiliteit van de Q-waarde-schattingen.  \n",
    "* **Discount factor (γ):** Legt de nadruk op toekomstige ten opzichte van directe beloningen; een hogere waarde stimuleert langetermijnplanning, terwijl een lagere waarde kortetermijndoelen voorrang geeft.  \n",
    "\n",
    "De resultaten van deze experimenten zullen worden gedocumenteerd, waarbij de impact van elke parameterkeuze op de convergentiesnelheid, de stabiliteit van de training (bijvoorbeeld via reward-curves) en de uiteindelijke prestaties van de agent wordt geanalyseerd. Dit proces helpt bij het identificeren van een robuuste set hyperparameters voor de **“Warlords”**-omgeving.\n",
    "\n",
    "\n",
    "### Vergelijking met Baseline Agent\n",
    "\n",
    "Om de effectiviteit en het leervermogen van de geïmplementeerde DQN-agent te valideren, zullen zijn prestaties worden vergeleken met een **baseline agent**. Deze baseline agent zal een eenvoudige strategie volgen, namelijk het **uitvoeren van willekeurige, valide acties** binnen de \"Warlords\" omgeving.\n",
    "\n",
    "Door de prestaties van de DQN-agent (bijvoorbeeld gemeten aan de hand van de gemiddelde behaalde score of beloning per episode over een significant aantal evaluatie-episodes) af te zetten tegen de prestaties van de random baseline, kan objectief worden vastgesteld in welke mate de DQN-agent daadwerkelijk een effectieve strategie heeft geleerd. Dit biedt een kwantitatieve maatstaf voor het succes van de gekozen aanpak en de getrainde modellen. De progressie van de DQN-agent gedurende de training (bijvoorbeeld via reward curves) zal ook worden geanalyseerd om het leerproces te visualiseren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617fcb2a",
   "metadata": {},
   "source": [
    "# Bronnen\n",
    "[1] Wikipedia contributors. (2025, April 5). Warlords (1980 video game). Wikipedia. https://en.wikipedia.org/wiki/Warlords_(1980_video_game)\n",
    "\n",
    "[2] The Deep Q-Network (DQN) - Hugging face Deep RL course. (n.d.-b). https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-network#:~:text=Why%20do%20we,those%20frames.\n",
    "\n",
    "[3] The Deep Q-Learning Algorithm - Hugging Face Deep RL course. (n.d.). https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm#:~:text=go%20through%20them!-,Experience%20Replay%20to%20make%20more%20efficient%20use%20of%20experiences,observation%20sequences%20and%20avoid%C2%A0action%20values%20from%20oscillating%20or%20diverging%20catastrophically.,-In%20the%20Deep\n",
    "\n",
    "[4] Amin, S. (2025, January 30). Deep Q-Learning (DQN) - Samina Amin - Medium. Medium. https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae#:~:text=Key%20Concepts%20of,to%20Q%2DLearning%3A\n",
    "\n",
    "[5] Amit, H. (2025, April 18). Deep Reinforcement Learning with Experience Replay - Hey Amit - Medium. Medium. https://medium.com/@heyamit10/deep-reinforcement-learning-with-experience-replay-1222ea711897#:~:text=Balancing%20Exploration%20and,agent%E2%80%99s%20policy%20converges.\n",
    "\n",
    "[6] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013, December 19). Playing Atari with Deep Reinforcement Learning. arXiv.org. https://arxiv.org/abs/1312.5602\n",
    "\n",
    "[7] Lee, S. (n.d.). Eight tips for optimizing DQN models in RL. https://www.numberanalytics.com/blog/optimizing-dqn-models-in-rl#hyperparameter-tuning-strategies:~:text=1.%20Hyperparameter%20Tuning,long%E2%80%90term%20gain"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
